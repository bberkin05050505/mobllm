import numpy as np
import utils
import time

from omegaconf import DictConfig
from mloggers import MultiLogger
from typing import List, Any, Dict, Tuple
from current_functions import CurrentFunctions
from optimizer import Optimizer
from scorers import ComplexityScorer
from sklearn.metrics import r2_score
from plotter import Plotter


class SR(object):
    """
    Symbolic regression clas for performing the SR phase. 
    """
    def __init__(self, cfg: DictConfig, logger: MultiLogger, cur_funcs: CurrentFunctions, sr_prompt: str, temp_sch: Any, results: Dict, 
                 model: Any, optimizer: Optimizer, scorer: ComplexityScorer, test_pts: np.ndarray, plotter: Plotter,
                 true_f: Any, output_path: str) -> None:
        """
        Initializes the ED phase.

        Parameters
        ----------
        cfg : DictConfig -> The configuration file.
        logger : MultiLogger -> The logger to log to.
        """
        self.cfg = cfg
        self.logger = logger

        self.current_functions = cur_funcs
        self.optimizer = optimizer
        self.scorer = scorer
        self.plotter = plotter
        self.true_function = true_f

        self.test_points = test_pts
        self.base_prompt = sr_prompt
        self.model = model

        self.temperature_scheduler = temp_sch
        self.temperature = cfg.model.temperature
        self.results = results
        self.output_path = output_path

        self.max_retries = cfg.max_retries
        self.force_valid = cfg.force_valid
        self.force_unique = cfg.force_unique
        self.num_variables = cfg.experiment.function.num_variables
        self.save_video = cfg.plotter.save_video
        self.save_frames = cfg.plotter.save_frames

    def get_new_function(self, prompt: str) -> Tuple[List, bool]:
        """
        Generates a new function from the model, given a prompt.

        Parameters
        ----------
        prompt -> the prompt to use for the model.

        Returns
        -------
        functions -> the new functions generated by the model as a string.
        found_valid -> whether a valid function was found.
        model_output -> the raw LLM output.
        """
        self.logger.info("Prompt: " + prompt)
        new_output = self.model.generate(prompt, return_prompt=False, image_files=None, temperature=self.temperature)
        self.logger.info("Model output: " + new_output)

        functions = []
        lines = new_output.split("\n")
        for line in lines:
            if "x" not in line:
                    self.logger.info(f"Skipping line {line} as it does not contain 'x' and is likely not a function.")  
                    continue
            if "Error" in line:
                self.logger.info(f"Skipping line {line} as it contains 'Error'.")
                continue
            line = utils.clean_function(line)
            if line == "":
                continue
            self.logger.info("Cleaned line: " + line + ".")
            try:
                valid, reason = utils.is_valid_function(line, self.current_functions, self.num_variables)
                self.logger.info(f"Valid: {valid}. Reason: {reason}.")
                if valid:
                    functions.append(line)
                elif not valid and reason == "Function already in prompt." and not self.force_unique:
                    functions.append(line)
            except Exception as e:
                self.logger.warning(f"Could not parse line {line}.")
                self.logger.warning(str(e))
                pass
        
        found_valid = False
        if len(functions) == 0:
            self.logger.warning("Could not find a valid function in the output. Using the last function in the output.")
            functions = [self.current_functions.get_best_function()]
        else:
            found_valid = True
            functions = [utils.string_to_function(function, self.num_variables) for function in functions]
            self.logger.info(f"Found functions: {functions}.")

        return functions, found_valid, new_output

    def get_new_function_and_score(self, prompt: str) -> Tuple[Any, Any, Any, float]:
        """
        Generates new functions from the model, given a prompt, and scores it if it is valid. Only returns the 
        best function found so far. If the model generates invalid functions, returns the best function found
        in the previous iterations. If the model generates valid functions, returns only the score of the best
        function that was generated in this iteration round.

        Parameters
        ----------
        prompt -> the prompt to use for the model.

        Returns
        -------
        expression -> the coefficient form of the function.
        function -> function with the optimized coefficients.
        popt -> the optimized parameters dict for the function.
        score -> the score of the function.
        model_output -> the raw LLM output.
        """
        valid = False
        start_time = time.perf_counter()
        for i in range(self.max_retries):
            self.logger.info(f"Attempt {i+1} of {self.max_retries} to find a valid function.")
            functions, valid, model_output = self.get_new_function(prompt)
            if valid and len(functions) >= 1:
                self.logger.info(f"Found {len(functions)} functions in the output.")
                break
            else:
                self.logger.info(f"Could not find a valid function in the output. Trying again.")

        self.results["tries_per_iteration"].append(i+1)
        self.results["times"]["generation_per_iteration"].append(time.perf_counter() - start_time)

        if not valid:
            if self.force_valid:
                self.logger.error(f"Could not find a valid function after {self.max_retries} tries. Exiting.")
                exit(1)
            else:
                best_expr = self.current_functions.get_best_function(return_coeff=True)
                best_function, best_popt = self.current_functions.functions[best_expr]
                best_score = self.current_functions.scores[best_expr]
                self.logger.warning(f"Could not find a valid function after {self.max_retries} tries. Using {best_function}.")
        else:
            best_score = np.inf
            start_time = time.perf_counter()
            for function in functions:
                if not self.current_functions.func_in_list(function):
                    self.results["num_unique"] += 1
                self.results["generations_per_iteration"].append(len(functions))
                try:
                    opt_function, exp, popt_dict = self.optimizer.optimize(function, return_coeff=True)
                    score = self.scorer.score(opt_function)
                    self.logger.info(f"New function: {str(opt_function)}. Score: {score}.")
                    if score < best_score:
                        best_score = score
                        best_function = opt_function
                        best_expr = exp
                        best_popt = popt_dict
                except Exception as e:
                    self.logger.warning(f"Could not optimize function {function}. {e}")
                    pass

            self.results["times"]["optimization_per_iteration"].append(time.perf_counter() - start_time)
            self.logger.info(f"Optimizer time: {time.perf_counter() - start_time}.")
            if best_score == np.inf:
                self.logger.warning(f"No functions scored below inf. Using the best function in the prompt.")
                best_expr = self.current_functions.get_best_function(return_coeff=True)
                best_function, best_popt = self.current_functions.functions[best_expr]
                best_score = self.current_functions.scores[best_expr]
            self.logger.info(f"Best function: {best_function}. Score: {best_score}.")

        self.logger.info(f"Finished get new function and score. Best function: {best_function}. Score: {best_score}.")
        return best_expr, best_function, best_popt, best_score, model_output
    
    def get_R2_scores(self, function: Any, train_points: np.ndarray, test_points: np.ndarray) -> Tuple[float, float, float]:
        """
        Computes the R2 scores for the train and test sets given a function, removing the 5% worst predictions.
        
        Parameters
        ----------
        function -> the function to evaluate.
        y_true_train -> train points.
        y_true_test -> test points.
        
        Returns
        -------
        r2_train -> the R2 score for the train set.
        r2_test -> the R2 score for the test set.
        r2_all -> the R2 score for all points.
        """        
        def compute_predictions(function, x_points, y_true, num_variables):
            y_pred = utils.eval_function(function, x_points, num_variables)
            
            # Compute a boolean mask of the 5% worst predictions to remove them
            # If number of data points is few, manually remove at least one index
            indices_to_remove = min(-1, -int(len(x_points) * 0.05))

            # Returns a slice of the last "indices_to_remove" elements of the np.argsort() array
            # utils.eval_function returns a (num_data_pts, ) shape. So y_true must have that shape
            # as well, for the y_pred - y_true to work as intended
            worst_indices = np.argsort(np.abs(y_pred - y_true))[indices_to_remove:]
            mask = np.zeros(len(x_points), dtype=bool)
            mask[worst_indices] = True
            
            return y_pred[~mask], mask
        
        # Get all the rows (all the data), and all columns except the last one to exclude y values
        # Compute_predictions assumes (num_data_pts, num_variables) shape, so we reshape
        x_true_train = train_points[:, :-1].reshape(-1, self.num_variables)
        x_true_test = test_points[:, :-1].reshape(-1, self.num_variables)
        
        y_true_train = train_points[:, -1]
        y_true_test = test_points[:, -1]
        
        try:
            y_pred_train, y_train_mask = compute_predictions(function, x_true_train, y_true_train, self.num_variables)
            y_pred_test, y_test_mask = compute_predictions(function, x_true_test, y_true_test, self.num_variables)

            y_true_train = y_true_train[~y_train_mask]
            y_true_test = y_true_test[~y_test_mask]
            
            assert len(y_true_train) == len(y_pred_train), f"Length of true train points ({len(y_true_train)}) does not match length of predicted train points ({len(y_pred_train)})."
            assert len(y_true_test) == len(y_pred_test), f"Length of true test points ({len(y_true_test)}) does not match length of predicted test points ({len(y_pred_test)})."
            
        except Exception as e:
            self.logger.warning(f"Could not evaluate function {function}. {e}")
            return np.nan, np.nan, np.nan
        try:
            r2_train = r2_score(y_true_train, y_pred_train)
        except Exception as e:
            self.logger.warning(f"Could not calculate R2 score for train set. {e}")
            r2_train = np.nan
        try:
            r2_test = r2_score(y_true_test, y_pred_test)
        except Exception as e: 
            self.logger.warning(f"Could not calculate R2 score for test set. {e}")
            r2_test = np.nan
        try:
            r2_all = r2_score(np.concatenate((y_true_train, y_true_test)), np.concatenate((y_pred_train, y_pred_test)))
        except Exception as e:
            self.logger.warning(f"Could not calculate R2 score for all points. {e}")
            r2_all = np.nan

        return r2_train, r2_test, r2_all

    
    def propose_exp_and_get_score(self, info: str, iteration: int, frames: List) -> List[str]:
        """
        Prompt the LLM for proposing symbolic expressions, given info to be included from past iterations.
        For the base SR algorithm, info contains the past (x, f(x)) points. 
        
        iteration is to keep track of which iteration we are performing for logging purposes. 

        Returns
        -------
        expression -> the coefficient form of the best-scoring function. This includes both the newly proposed functions and the best-scoring function from the previous iterations.
        function -> best-scoring function with the optimized coefficients.
        score -> the score of the function as a float.
        model_output -> the raw LLM output.
        """
        start_time = time.perf_counter()

        # format the prompt to include all the data points observed so far
        prompt = self.base_prompt.format(points=info, functions="{functions}")

        # format the prompt to include the c lowest-cost functions so far
        prompt = self.current_functions.get_prompt(prompt)

        self.logger.info(f"SR Round {iteration + 1}.")
        self.logger.info(f"Scores: {self.current_functions.scores}.")

        # Handle temperature schedule
        if self.temperature_scheduler is not None:
            self.temperature = self.temperature_scheduler.get_last_lr()[0]
            self.logger.info(f"Temperature: {self.temperature}.")
            self.results["temperatures"].append(self.temperature)
            self.temperature_scheduler.step()

        # Get new function and score
        expr, function, popt, score, model_output = self.get_new_function_and_score(prompt)
        self.current_functions.add_function(expr, function, popt)
        best_expr = self.current_functions.get_best_function(return_coeff=True)
        best_function, best_popt = self.current_functions.functions[best_expr]

        # Update results
        if expr == best_expr:
            self.results["best_found_at"] = iteration + 1
        self.results["iterations"] = iteration + 1
        self.results["scores"].append(score) if score != np.inf else self.results["scores"].append("inf")
        self.results["best_scores"].append(self.current_functions.scores[best_expr])
        self.results["best_scores_normalized"].append(self.current_functions.norm_scores[best_expr])
        self.results["times"]["iteration"].append(time.perf_counter() - start_time)

        # Compute R2 values
        train_points = utils.string_to_array(info)
        test_points = self.test_points
        r2_train, r2_test, r2_all = self.get_R2_scores(function, train_points, test_points)
        self.results["R2_trains"].append(r2_train)
        self.results["R2_tests"].append(r2_test)
        self.results["R2_alls"].append(r2_all)

        # Update video
        if self.save_video:
            if not score == np.inf:
                frame, ax = self.plotter.record_frame(best_function, function, r2_test, self.true_function, iteration, plot_true=True)
                if self.save_frames:
                    frame.savefig(self.output_path + "frames/" + f"{iteration + 1}.png")
                frames.append(frame)
            else:
                self.logger.warning(f"Skipping frame {iteration + 1} as the score is inf.")
        
        best_score = self.current_functions.get_best_score()
        return best_expr, best_function, best_score, model_output
        
    

    